@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@misc{Authors14,
 author = {Full Author Name},
 title = {The Frobnicatable Foo Filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {Full Author Name},
 title = {Frobnication Tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {Alvin Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe},
title = {Frobnication Revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {Alvin Alpher and Ferris P.~N. Fotheringham-Smythe and Gavin Gamow},
title = {Can a Machine Frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@misc{vanamsterdam2020,
      title={Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction}, 
      author={Beatrice van Amsterdam and Matthew J. Clarkson and Danail Stoyanov},
      year={2020},
      eprint={2003.04772},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{becattini2017,
  doi = {10.48550/ARXIV.1705.01781},
  url = {https://arxiv.org/abs/1705.01781},
  author = {Becattini, Federico and Uricchio, Tiberio and Seidenari, Lorenzo and Ballan, Lamberto and Del Bimbo, Alberto},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Am I Done? Predicting Action Progress in Videos},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{twinanda2016,
      title={EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos}, 
      author={Andru Putra Twinanda and Sherif Shehata and Didier Mutter and Jacques Marescaux and Michel de Mathelin and Nicolas Padoy},
      year={2016},
      eprint={1602.03012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{twinanda2019,
	doi = {10.1109/tmi.2018.2878055},
	url = {https://doi.org/10.1109%2Ftmi.2018.2878055},
	year = 2019,
	month = {apr},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {38},
	number = {4},
	pages = {1069--1078},
	author = {Andru Putra Twinanda and Gaurav Yengera and Didier Mutter and Jacques Marescaux and Nicolas Padoy},
	title = {{RSDNet}: Learning to Predict Remaining Surgery Duration from Laparoscopic Videos Without Manual Annotations},
	journal = {{IEEE} Transactions on Medical Imaging}
}

@misc{kukleva2019,
      title={Unsupervised learning of action classes with continuous temporal embedding}, 
      author={Anna Kukleva and Hilde Kuehne and Fadime Sener and Juergen Gall},
      year={2019},
      eprint={1904.04189},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{nwoye2022,
	doi = {10.1016/j.media.2022.102433},
	url = {https://doi.org/10.1016%2Fj.media.2022.102433},
	year = 2022,
	month = {may},
	publisher = {Elsevier {BV}},
  
	volume = {78},
  
	pages = {102433},
  
	author = {Chinedu Innocent Nwoye and Tong Yu and Cristians Gonzalez and Barbara Seeliger and Pietro Mascagni and Didier Mutter and Jacques Marescaux and Nicolas Padoy},
  
	title = {Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos},
  
	journal = {Medical Image Analysis}
}

@misc{han2017,
      title={Human Action Forecasting by Learning Task Grammars}, 
      author={Tengda Han and Jue Wang and Anoop Cherian and Stephen Gould},
      year={2017},
      eprint={1709.06391},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gao2019,
      title={StartNet: Online Detection of Action Start in Untrimmed Videos}, 
      author={Mingfei Gao and Mingze Xu and Larry S. Davis and Richard Socher and Caiming Xiong},
      year={2019},
      eprint={1903.09868},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ghoddoosian2020,
      title={Action Duration Prediction for Segment-Level Alignment of Weakly-Labeled Videos}, 
      author={Reza Ghoddoosian and Saif Sayed and Vassilis Athitsos},
      year={2020},
      eprint={2011.10190},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hu2019,
      title={Progress Regression RNN for Online Spatial-Temporal Action Localization in Unconstrained Videos}, 
      author={Bo Hu and Jianfei Cai and Tat-Jen Cham and Junsong Yuan},
      year={2019},
      eprint={1903.00304},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

    @inproceedings{heidarivincheh2016,
        	title={Beyond Action Recognition: Action Completion in RGB-D Data},
        	author={Farnoosh Heidarivincheh, Majid Mirmehdi and Dima Damen},
        	year={2016},
        	month={September},
        	pages={142.1-142.11},
        	articleno={142},
        	numpages={11},
        	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
        	publisher={BMVA Press},
        	editor={Richard C. Wilson, Edwin R. Hancock and William A. P. Smith},
        	doi={10.5244/C.30.142},
        	isbn={1-901725-59-6},
        	url={https://dx.doi.org/10.5244/C.30.142}
        }


@misc{heidarivincheh2018,
      title={Action Completion: A Temporal Model for Moment Detection}, 
      author={Farnoosh Heidarivincheh and Majid Mirmehdi and Dima Damen},
      year={2018},
      eprint={1805.06749},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{heidarivincheh2019,
  author={Heidarivincheh, Farnoosh and Mirmehdi, Majid and Damen, Dima},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)}, 
  title={Weakly-Supervised Completion Moment Detection using Temporal Attention}, 
  year={2019},
  volume={},
  number={},
  pages={1188-1196},
  doi={10.1109/ICCVW.2019.00150}}


@misc{li2017,
      title={Progress Estimation and Phase Detection for Sequential Processes}, 
      author={Xinyu Li and Yanyi Zhang and Jianyu Zhang and Yueyang Chen and Shuhong Chen and Yue Gu and Moliang Zhou and Richard A. Farneth and Ivan Marsic and Randall S. Burd},
      year={2017},
      eprint={1702.08623},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2023,
      title={Rich Action-semantic Consistent Knowledge for Early Action Prediction}, 
      author={Xiaoli Liu and Jianqin Yin and Di Guo},
      year={2023},
      eprint={2201.09169},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{marafioti2021,
      title={CataNet: Predicting remaining cataract surgery duration}, 
      author={Andrés Marafioti and Michel Hayoz and Mathias Gallardo and Pablo Márquez Neila and Sebastian Wolf and Martin Zinkernagel and Raphael Sznitman},
      year={2021},
      eprint={2106.11048},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{yengera2018,
      title={Less is More: Surgical Phase Recognition with Less Annotations through Self-Supervised Pre-training of CNN-LSTM Networks}, 
      author={Gaurav Yengera and Didier Mutter and Jacques Marescaux and Nicolas Padoy},
      year={2018},
      eprint={1805.08569},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{ma2016,
author = {Ma, Shugao and Sigal, Leonid and Sclaroff, Stan},
title = {Learning Activity Progression in LSTMs for Activity Detection and Early Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@misc{benaim2020,
      title={SpeedNet: Learning the Speediness in Videos}, 
      author={Sagie Benaim and Ariel Ephrat and Oran Lang and Inbar Mosseri and William T. Freeman and Michael Rubinstein and Michal Irani and Tali Dekel},
      year={2020},
      eprint={2004.06130},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gao2019,
      title={StartNet: Online Detection of Action Start in Untrimmed Videos}, 
      author={Mingfei Gao and Mingze Xu and Larry S. Davis and Richard Socher and Caiming Xiong},
      year={2019},
      eprint={1903.09868},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jenni2020,
      title={Video-ReTime: Learning Temporally Varying Speediness for Time Remapping}, 
      author={Simon Jenni and Markus Woodson and Fabian Caba Heilbron},
      year={2022},
      eprint={2205.05609},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{pickup2014,
  author={Pickup, Lyndsey C. and Pan, Zheng and Wei, Donglai and Shih, YiChang and Zhang, Changshui and Zisserman, Andrew and Scholkopf, Bernhard and Freeman, William T.},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Seeing the Arrow of Time}, 
  year={2014},
  volume={},
  number={},
  pages={2043-2050},
  doi={10.1109/CVPR.2014.262}}


@inproceedings{wei2018,
  title={Learning and Using the Arrow of Time},
  author={Wei, Donglai and Lim, Joseph J and Zisserman, Andrew and Freeman, William T},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8052--8060},
  year={2018}
}

@misc{simonyan2015,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{aksamentov2017,
  title={Deep Neural Networks Predict Remaining Surgery Duration from Cholecystectomy Videos},
  author={Ivan Aksamentov and Andru Putra Twinanda and Didier Mutter and Jacques Marescaux and Nicolas Padoy},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year={2017}
}

@misc{he2015,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@incollection{he2014,
	doi = {10.1007/978-3-319-10578-9_23},
	url = {https://doi.org/10.1007%2F978-3-319-10578-9_23},
	year = 2014,
	publisher = {Springer International Publishing},
	pages = {346--361},
	author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	booktitle = {Computer Vision {\textendash} {ECCV} 2014}
}

@misc{girshick2015,
      title={Fast R-CNN}, 
      author={Ross Girshick},
      year={2015},
      eprint={1504.08083},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{liu2023lovit,
      title={LoViT: Long Video Transformer for Surgical Phase Recognition}, 
      author={Yang Liu and Maxence Boels and Luis C. Garcia-Peraza-Herrera and Tom Vercauteren and Prokar Dasgupta and Alejandro Granados and Sebastien Ourselin},
      year={2023},
      eprint={2305.08989},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jamal2023,
      title={SurgMAE: Masked Autoencoders for Long Surgical Video Analysis}, 
      author={Muhammad Abdullah Jamal and Omid Mohareri},
      year={2023},
      eprint={2305.11451},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@Article{pucci2023,
AUTHOR = {Pucci, Davide and Becattini, Federico and Del Bimbo, Alberto},
TITLE = {Joint-Based Action Progress Prediction},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {1},
ARTICLE-NUMBER = {520},
URL = {https://www.mdpi.com/1424-8220/23/1/520},
PubMedID = {36617115},
ISSN = {1424-8220},
ABSTRACT = {Action understanding is a fundamental computer vision branch for several applications, ranging from surveillance to robotics. Most works deal with localizing and recognizing the action in both time and space, without providing a characterization of its evolution. Recent works have addressed the prediction of action progress, which is an estimate of how far the action has advanced as it is performed. In this paper, we propose to predict action progress using a different modality compared to previous methods: body joints. Human body joints carry very precise information about human poses, which we believe are a much more lightweight and effective way of characterizing actions and therefore their execution. Estimating action progress can in fact be determined based on the understanding of how key poses follow each other during the development of an activity. We show how an action progress prediction model can exploit body joints and integrate it with modules providing keypoint and action information in order to be run directly from raw pixels. The proposed method is experimentally validated on the Penn Action Dataset.},
DOI = {10.3390/s23010520}
}





% Dataset Citations
@InProceedings{kuehne2014,
   author= {Hilde Kuehne and Arslan, A. B. and Serre, T.},
   title = {The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities},
   booktitle = {Proceedings of Computer Vision and Pattern Recognition Conference (CVPR)},
   year = {2014},
}

@InProceedings{kuehne2016,
author = {Hilde Kuehne and Juergen Gall and Thomas Serre},
title = {An end-to-end generative framework for video segmentation and recognition},
booktitle = {Proc. IEEE Winter Applications of Computer Vision Conference (WACV 16)},
year = {2016},
month = {Mar},
address = {Lake Placid},
}

@misc{soomro2012,
      title={UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild}, 
      author={Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},
      year={2012},
      eprint={1212.0402},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{carreira2018,
      title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset}, 
      author={Joao Carreira and Andrew Zisserman},
      year={2018},
      eprint={1705.07750},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2023,
    title={Real-time estimation of the remaining surgery duration for cataract surgery using deep convolutional neural networks and long short-term memory},
    author={Bowen Wang and Liangzhi Li and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara},
    year={2023},
    booktitle = {BMC Med Inform Decis Mak 23, 80}
}

% improved dense trajectories
@INPROCEEDINGS{wang2013,
  author={Wang, Heng and Schmid, Cordelia},
  booktitle={2013 IEEE International Conference on Computer Vision}, 
  title={Action Recognition with Improved Trajectories}, 
  year={2013},
  volume={},
  number={},
  pages={3551-3558},
  doi={10.1109/ICCV.2013.441}
}


% action tubes
@misc{gkioxari2014,
      title={Finding Action Tubes}, 
      author={Georgia Gkioxari and Jitendra Malik},
      year={2014},
      eprint={1411.6031},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{arnab2021,
      title={ViViT: A Video Vision Transformer}, 
      author={Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lučić and Cordelia Schmid},
      year={2021},
      eprint={2103.15691},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{tran2015,
      title={Learning Spatiotemporal Features with 3D Convolutional Networks}, 
      author={Du Tran and Lubomir Bourdev and Rob Fergus and Lorenzo Torresani and Manohar Paluri},
      year={2015},
      eprint={1412.0767},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vidalmata2020,
      title={Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences}, 
      author={Rosaura G. VidalMata and Walter J. Scheirer and Anna Kukleva and David Cox and Hilde Kuehne},
      year={2020},
      eprint={2001.11122},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{redmon2016,
      title={YOLO9000: Better, Faster, Stronger}, 
      author={Joseph Redmon and Ali Farhadi},
      year={2016},
      eprint={1612.08242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}